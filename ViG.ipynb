{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f13d9f-12a6-4f11-813e-2b7a7e31290b",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9fe7e-d5ad-4ddb-ae46-1395626afb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556cf7a-4c7d-40ad-be93-1b3ec1322fe2",
   "metadata": {},
   "source": [
    "**Creating Edges of the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea393ca-8a8d-406e-8a71-8f8e63f4d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(x):\n",
    "    \"\"\"\n",
    "    Compute pairwise distance of a point cloud.\n",
    "    Args:\n",
    "        x: tensor (batch_size, num_points, num_dims)\n",
    "    Returns:\n",
    "        pairwise distance: (batch_size, num_points, num_points)\n",
    "    \"\"\"\n",
    "    with tf.name_scope('pairwise_distance'):\n",
    "        x_inner = -2 * tf.matmul(x, tf.transpose(x, perm=[0, 2, 1]))\n",
    "        x_square = tf.reduce_sum(tf.square(x), axis=-1, keepdims=True)\n",
    "        return x_square + x_inner + tf.transpose(x_square, perm=[0, 2, 1])\n",
    "\n",
    "def dense_knn_matrix(x, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with tf.name_scope('dense_knn_matrix'):\n",
    "        x = tf.squeeze(tf.transpose(x, perm=[0, 2, 1, 3]), axis=-1)\n",
    "        batch_size, n_points, n_dims = tf.unstack(tf.shape(x))\n",
    "        ### memory efficient implementation ###\n",
    "        n_part = 10000\n",
    "        if n_points > n_part:\n",
    "            nn_idx_list = []\n",
    "            groups = tf.math.ceil(n_points / n_part)\n",
    "            for i in tf.range(groups):\n",
    "                start_idx = n_part * i\n",
    "                end_idx = tf.minimum(n_points, n_part * (i + 1))\n",
    "                dist = part_pairwise_distance(x, start_idx, end_idx)\n",
    "                if relative_pos is not None:\n",
    "                    dist += relative_pos[:, start_idx:end_idx]\n",
    "                _, nn_idx_part = tf.math.top_k(-dist, k=k)\n",
    "                nn_idx_list += [nn_idx_part]\n",
    "            nn_idx = tf.concat(nn_idx_list, axis=1)\n",
    "        else:\n",
    "            dist = pairwise_distance(x)\n",
    "            if relative_pos is not None:\n",
    "                dist += relative_pos\n",
    "            _, nn_idx = tf.math.top_k(-dist, k=k)\n",
    "        ######\n",
    "        center_idx = tf.transpose(tf.tile(tf.expand_dims(tf.range(0, n_points), axis=0), [batch_size, k, 1]), perm=[0, 2, 1])\n",
    "    return tf.stack((nn_idx, center_idx), axis=0)\n",
    "\n",
    "def xy_dense_knn_matrix(x, y, k=16, relative_pos=None):\n",
    "    \"\"\"Get KNN based on the pairwise distance.\n",
    "    Args:\n",
    "        x: (batch_size, num_dims, num_points, 1)\n",
    "        k: int\n",
    "    Returns:\n",
    "        nearest neighbors: (batch_size, num_points, k) (batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    with tf.name_scope('xy_dense_knn_matrix'):\n",
    "        x = tf.squeeze(tf.transpose(x, perm=[0, 2, 1, 3]), axis=-1)\n",
    "        y = tf.squeeze(tf.transpose(y, perm=[0, 2, 1, 3]), axis=-1)\n",
    "        batch_size, n_points, n_dims = tf.unstack(tf.shape(x))\n",
    "        dist = xy_pairwise_distance(x, y)\n",
    "        if relative_pos is not None:\n",
    "            dist += relative_pos\n",
    "        _, nn_idx = tf.math.top_k(-dist, k=k)\n",
    "        center_idx = tf.transpose(tf.tile(tf.expand_dims(tf.range(0, n_points), axis=0), [batch_size, k, 1]), perm=[0, 2, 1])\n",
    "    return tf.stack((nn_idx, center_idx), axis=0)\n",
    "\n",
    "class DenseDilated(tf.Module):\n",
    "    \"\"\"\n",
    "    Find dilated neighbor from neighbor list\n",
    "\n",
    "    edge_index: (2, batch_size, num_points, k)\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilated, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "\n",
    "    def __call__(self, edge_index):\n",
    "        with tf.name_scope('DenseDilated'):\n",
    "            if self.stochastic:\n",
    "                if tf.random.uniform(shape=()) < self.epsilon and tf.keras.backend.learning_phase():\n",
    "                    num = self.k * self.dilation\n",
    "                    randnum = tf.random.shuffle(tf.range(num))[:self.k]\n",
    "                    edge_index = tf.gather(edge_index, randnum, axis=-1)\n",
    "                else:\n",
    "                    edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "            else:\n",
    "                edge_index = edge_index[:, :, :, ::self.dilation]\n",
    "            return edge_index\n",
    "\n",
    "class DenseDilatedKnnGraph(tf.Module):\n",
    "    \"\"\"\n",
    "    Find the neighbors' indices based on dilated knn\n",
    "    \"\"\"\n",
    "    def __init__(self, k=9, dilation=1, stochastic=False, epsilon=0.0):\n",
    "        super(DenseDilatedKnnGraph, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.stochastic = stochastic\n",
    "        self.epsilon = epsilon\n",
    "        self.k = k\n",
    "        self._dilated = DenseDilated(k, dilation, stochastic, epsilon)\n",
    "\n",
    "    def __call__(self, x, y=None, relative_pos=None):\n",
    "        with tf.name_scope('DenseDilatedKnnGraph'):\n",
    "            if y is not None:\n",
    "                #### normalize\n",
    "                x = tf.nn.l2_normalize(x, axis=1)\n",
    "                y = tf.nn.l2_normalize(y, axis=1)\n",
    "                ####\n",
    "                edge_index = xy_dense_knn_matrix(x, y, self.k * self.dilation, relative_pos)\n",
    "            else:\n",
    "                #### normalize\n",
    "                x = tf.nn.l2_normalize(x, axis=1)\n",
    "                ####\n",
    "                edge_index = dense_knn_matrix(x, self.k * self.dilation, relative_pos)\n",
    "            return self._dilated(edge_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daa579e-6795-4f4d-b88d-b9fbf6bb3e37",
   "metadata": {},
   "source": [
    "**Creating Vertices of the Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e33564-038d-4134-9ab7-daa5377350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(layers.Layer):\n",
    "    def __init__(self, filters, activation='relu', norm=None, use_bias=True):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.conv = layers.Conv2D(filters=filters[1], kernel_size=1, strides=1, padding='valid', use_bias=use_bias)\n",
    "        self.batch_norm = layers.BatchNormalization() if norm == 'batch' else None\n",
    "        self.activation = activation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        if self.batch_norm:\n",
    "            x = self.batch_norm(x)\n",
    "        if self.activation == 'relu':\n",
    "            x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "def batched_index_select(x, indices):\n",
    "    batch_size = tf.shape(indices)[0]\n",
    "    batch_indices = tf.range(0, batch_size)\n",
    "    indices = tf.stack([batch_indices, indices], axis=1)\n",
    "    return tf.gather_nd(x, indices)\n",
    "\n",
    "class MRConv2d(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', norm=None, use_bias=True):\n",
    "        super(MRConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], activation, norm, use_bias)\n",
    "\n",
    "    def call(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = tf.math.reduce_max(x_j - x_i, axis=-1, keepdims=True)\n",
    "        b, c, n, _ = x.shape\n",
    "        x = tf.concat([tf.expand_dims(x, axis=2), tf.expand_dims(x_j, axis=2)], axis=2)\n",
    "        x = tf.reshape(x, [b, 2 * c, n, _])\n",
    "        return self.nn(x)\n",
    "\n",
    "class EdgeConv2d(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', norm=None, use_bias=True):\n",
    "        super(EdgeConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels*2, out_channels], activation, norm, use_bias)\n",
    "\n",
    "    def call(self, x, edge_index, y=None):\n",
    "        x_i = batched_index_select(x, edge_index[1])\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        max_value = tf.math.reduce_max(self.nn(tf.concat([x_i, x_j - x_i], axis=1)), axis=-1, keepdims=True)\n",
    "        return max_value\n",
    "\n",
    "class GraphSAGE(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', norm=None, use_bias=True):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.nn1 = BasicConv([in_channels, in_channels], activation, norm, use_bias)\n",
    "        self.nn2 = BasicConv([in_channels*2, out_channels], activation, norm, use_bias)\n",
    "\n",
    "    def call(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j, _ = tf.math.reduce_max(self.nn1(x_j), axis=-1, keepdims=True)\n",
    "        return self.nn2(tf.concat([x, x_j], axis=1))\n",
    "\n",
    "class GINConv2d(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu', norm=None, use_bias=True):\n",
    "        super(GINConv2d, self).__init__()\n",
    "        self.nn = BasicConv([in_channels, out_channels], activation, norm, use_bias)\n",
    "        eps_init = 0.0\n",
    "        self.eps = tf.Variable(initial_value=[eps_init], trainable=True)\n",
    "\n",
    "    def call(self, x, edge_index, y=None):\n",
    "        if y is not None:\n",
    "            x_j = batched_index_select(y, edge_index[0])\n",
    "        else:\n",
    "            x_j = batched_index_select(x, edge_index[0])\n",
    "        x_j = tf.math.reduce_sum(x_j, axis=-1, keepdims=True)\n",
    "        return self.nn((1 + self.eps) * x + x_j)\n",
    "\n",
    "class GraphConv2d(layers.Layer):\n",
    "    def __init__(self, in_channels, out_channels, conv='edge', activation='relu', norm=None, use_bias=True):\n",
    "        super(GraphConv2d, self).__init__()\n",
    "        if conv == 'edge':\n",
    "            self.gconv = EdgeConv2d(in_channels, out_channels, activation, norm, use_bias)\n",
    "        elif conv == 'mr':\n",
    "            self.gconv = MRConv2d(in_channels, out_channels, activation, norm, use_bias)\n",
    "        elif conv == 'sage':\n",
    "            self.gconv = GraphSAGE(in_channels, out_channels, activation, norm, use_bias)\n",
    "        elif conv == 'gin':\n",
    "            self.gconv = GINConv2d(in_channels, out_channels, activation, norm, use_bias)\n",
    "        else:\n",
    "            raise NotImplementedError('conv:{} is not supported'.format(conv))\n",
    "\n",
    "    def call(self, x, edge_index, y=None):\n",
    "        return self.gconv(x, edge_index, y)\n",
    "\n",
    "class DyGraphConv2d(GraphConv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=9, dilation=1, conv='edge', activation='relu',\n",
    "                 norm=None, use_bias=True, stochastic=False, epsilon=0.0, r=1):\n",
    "        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, activation, norm, use_bias)\n",
    "        self.k = kernel_size\n",
    "        self.d = dilation\n",
    "        self.r = r\n",
    "        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)\n",
    "\n",
    "    def call(self, x, relative_pos=None):\n",
    "        B, C, H, W = x.shape\n",
    "        y = None\n",
    "        if self.r > 1:\n",
    "            y = tf.nn.avg_pool2d(x, self.r, self.r)\n",
    "            y = tf.reshape(y, [B, C, -1, 1])\n",
    "        x = tf.reshape(x, [B, C, -1, 1])\n",
    "        edge_index = self.dilated_knn_graph(x, y, relative_pos)\n",
    "        x = super(DyGraphConv2d, self).call(x, edge_index, y)\n",
    "        return tf.reshape(x, [B, -1, H, W])\n",
    "\n",
    "class Grapher(layers.Layer):\n",
    "    def __init__(self, in_channels, kernel_size=9, dilation=1, conv='edge', activation='relu', norm=None,\n",
    "                 use_bias=True, stochastic=False, epsilon=0.0, r=1, n=196, drop_path=0.0, relative_pos=False):\n",
    "        super(Grapher, self).__init__()\n",
    "        self.channels = in_channels\n",
    "        self.n = n\n",
    "        self.r = r\n",
    "        self.fc1 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=in_channels, kernel_size=1, strides=1, padding='valid', use_bias=use_bias),\n",
    "            layers.BatchNormalization() if norm == 'batch' else None\n",
    "        ])\n",
    "        self.graph_conv = DyGraphConv2d(in_channels, in_channels * 2, kernel_size, dilation, conv,\n",
    "                                        activation, norm, use_bias, stochastic, epsilon, r)\n",
    "        self.fc2 = tf.keras.Sequential([\n",
    "            layers.Conv2D(filters=in_channels, kernel_size=1, strides=1, padding='valid', use_bias=use_bias),\n",
    "            layers.BatchNormalization() if norm == 'batch' else None\n",
    "        ])\n",
    "        self.drop_path = layers.Dropout(drop_path) if drop_path > 0.0 else tf.keras.layers.Identity()\n",
    "        self.relative_pos = None\n",
    "        if relative_pos:\n",
    "            print('using relative_pos')\n",
    "            relative_pos_tensor = tf.convert_to_tensor(np.float32(get_2d_relative_pos_embed(in_channels,\n",
    "                int(n**0.5)))).unsqueeze(0).unsqueeze(1)\n",
    "            relative_pos_tensor = tf.image.resize(relative_pos_tensor, (n, n//(r*r)), method='bicubic', antialias=False)\n",
    "            self.relative_pos = tf.Variable(-relative_pos_tensor.squeeze(1), trainable=False)\n",
    "\n",
    "    def _get_relative_pos(self, relative_pos, H, W):\n",
    "        if relative_pos is None or H * W == self.n:\n",
    "            return relative_pos\n",
    "        else:\n",
    "            N = H * W\n",
    "            N_reduced = N // (self.r * self.r)\n",
    "            return tf.image.resize(relative_pos.unsqueeze(0), (N, N_reduced), method='bicubic')\n",
    "\n",
    "    def call(self, x):\n",
    "        _tmp = x\n",
    "        x = self.fc1(x)\n",
    "        B, C, H, W = x.shape\n",
    "        relative_pos = self._get_relative_pos(self.relative_pos, H, W)\n",
    "        x = self.graph_conv(x, relative_pos)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop_path(x) + _tmp\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9e18a-b172-47f2-ae4b-2cebecaafad3",
   "metadata": {},
   "source": [
    "**Main ViG Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32978dac-1c15-43fc-8027-76a39b541c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, activation='relu', drop_rate=0.0):\n",
    "        super(FFN, self).__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = tf.keras.layers.Conv2D(hidden_features, kernel_size=1, strides=1, padding='valid')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.activation = tf.keras.layers.Activation(activation)\n",
    "        self.fc2 = tf.keras.layers.Conv2D(out_features, kernel_size=1, strides=1, padding='valid')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.drop = tf.keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        shortcut = inputs\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.drop(x, training=training)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "class Stem(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_dim=768, activation='relu'):\n",
    "        super(Stem, self).__init__()\n",
    "        self.convs = [\n",
    "            tf.keras.layers.Conv2D(out_dim//8, kernel_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(activation),\n",
    "            tf.keras.layers.Conv2D(out_dim//4, kernel_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(activation),\n",
    "            tf.keras.layers.Conv2D(out_dim//2, kernel_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(activation),\n",
    "            tf.keras.layers.Conv2D(out_dim, kernel_size=3, strides=2, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(activation),\n",
    "            tf.keras.layers.Conv2D(out_dim, kernel_size=3, strides=1, padding='same'),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.stem_layers = tf.keras.Sequential(self.convs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.stem_layers(inputs)\n",
    "        return x\n",
    "\n",
    "class DeepGCN(tf.keras.Model):\n",
    "    def __init__(self, opt):\n",
    "        super(DeepGCN, self).__init__()\n",
    "        channels = opt.n_filters\n",
    "        k = opt.k\n",
    "        act = opt.act\n",
    "        norm = opt.norm\n",
    "        bias = opt.bias\n",
    "        epsilon = opt.epsilon\n",
    "        stochastic = opt.use_stochastic\n",
    "        conv = opt.conv\n",
    "        self.n_blocks = opt.n_blocks\n",
    "        drop_path = opt.drop_path\n",
    "        \n",
    "        self.stem = Stem(out_dim=channels, activation=act)\n",
    "\n",
    "        dpr = [tf.linspace(0, drop_path, self.n_blocks)]  # stochastic depth decay rule \n",
    "        num_knn = [tf.cast(x, tf.int32) for x in tf.linspace(k, 2*k, self.n_blocks)]  # number of knn's k\n",
    "        max_dilation = 196 // max(num_knn)\n",
    "        \n",
    "        self.pos_embed = tf.Variable(tf.zeros((1, 14, 14, channels)))\n",
    "\n",
    "        if opt.use_dilation:\n",
    "            self.backbone = [\n",
    "                tf.keras.Sequential([\n",
    "                    Grapher(channels, num_knn[i], min(i // 4 + 1, max_dilation), conv, act, norm,\n",
    "                                                bias, stochastic, epsilon, 1, drop_path=dpr[i]),\n",
    "                    FFN(channels, channels * 4, activation=act, drop_rate=dpr[i])\n",
    "                ]) for i in range(self.n_blocks)\n",
    "            ]\n",
    "        else:\n",
    "            self.backbone = [\n",
    "                tf.keras.Sequential([\n",
    "                    Grapher(channels, num_knn[i], 1, conv, act, norm,\n",
    "                                                bias, stochastic, epsilon, 1, drop_path=dpr[i]),\n",
    "                    FFN(channels, channels * 4, activation=act, drop_rate=dpr[i])\n",
    "                ]) for i in range(self.n_blocks)\n",
    "            ]\n",
    "\n",
    "        self.prediction = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1024, kernel_size=1, bias_initializer='zeros'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Activation(act),\n",
    "            tf.keras.layers.Dropout(opt.dropout),\n",
    "            tf.keras.layers.Conv2D(opt.n_classes, kernel_size=1, bias_initializer='zeros')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.stem(inputs) + self.pos_embed\n",
    "        B, H, W, C = x.shape\n",
    "        \n",
    "        for i in range(self.n_blocks):\n",
    "            x = self.backbone[i](x)\n",
    "\n",
    "        x = tf.reduce_mean(x, axis=[1, 2])\n",
    "        return self.prediction(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
